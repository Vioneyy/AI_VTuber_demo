"""
F5-TTS-Thai Engine (Real Implementation)
‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö API ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á F5-TTS-Thai
"""
import os
import asyncio
import subprocess
import numpy as np
import torch
import torchaudio
from io import BytesIO
import logging

logger = logging.getLogger(__name__)

class F5TTSThai:
    def __init__(self, device: str | None = None):
        # Device selection from .env or override; fallback to CUDA if available
        env_device = os.getenv("TTS_DEVICE")
        if device:
            self.device = device
        elif env_device:
            self.device = env_device
        else:
            # Map from global GPU preference when available
            try:
                from core.config import config as _cfg
                self.device = 'cuda' if _cfg.system.use_gpu else 'cpu'
            except Exception:
                self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # Safety: if CUDA requested but unavailable, fall back to CPU
        if self.device.startswith('cuda') and not torch.cuda.is_available():
            logger.warning("CUDA requested for F5-TTS but not available. Falling back to CPU.")
            self.device = 'cpu'

        self.use_reference = os.getenv("F5_TTS_USE_REFERENCE", "false").lower() == "true"
        # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡πá‡∏Å‡∏ï‡πå‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        self.ref_audio_path = os.getenv("TTS_REFERENCE_WAV", "reference_audio/jeed_voice.wav")
        self.ref_text = os.getenv("F5_TTS_REF_TEXT", "")
        self.speed = float(os.getenv("F5_TTS_SPEED", "1.0"))
        self.steps = int(os.getenv("F5_TTS_STEPS", "32"))  # default 32
        self.cfg_strength = float(os.getenv("F5_TTS_CFG_STRENGTH", "2.0"))
        self.sample_rate = int(os.getenv("F5_TTS_SAMPLE_RATE", "24000"))
        
        logger.info(f"F5-TTS-Thai: device={self.device}, speed={self.speed}, steps={self.steps}")
        
        try:
            # ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: ‡πÉ‡∏ä‡πâ TTS class ‡∏à‡∏≤‡∏Å f5_tts_th.tts
            from f5_tts_th.tts import TTS
            
            logger.info("üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î F5-TTS-Thai model...")
            
            # ‡πÇ‡∏´‡∏•‡∏î model ‡∏î‡πâ‡∏ß‡∏¢ TTS class
            # Note: F5 TTS-TH selects GPU automatically if available; we log chosen device.
            self.tts = TTS(model="v1")  # ‡πÉ‡∏ä‡πâ model v1 ‡∏ï‡∏≤‡∏° Hugging Face
            
            logger.info("‚úÖ F5-TTS-Thai ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

            
        except ImportError as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ import F5-TTS-Thai: {e}")
            logger.error("‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install f5-tts-thai")
            raise
        except Exception as e:
            logger.error(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î F5-TTS-Thai ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
            raise

    def set_use_reference(self, use_ref: bool):
        """‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î reference runtime"""
        self.use_reference = use_ref
        logger.info(f"F5-TTS: use_reference = {use_ref}")

    def _sanitize_text(self, text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        text = text.strip()
        import re
        # ‡∏•‡∏ö emoji ‡πÅ‡∏•‡∏∞ special characters
        text = re.sub(r'[^\w\s\u0E00-\u0E7F.,!?-]', '', text)
        return text

    def synthesize(self, text: str) -> bytes:
        """
        ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        """
        try:
            text = self._sanitize_text(text)
            
            if not text:
                logger.warning("‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á")
                return self._generate_silence(1.0)

            logger.info(f"üé§ F5-TTS-Thai ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {text[:50]}...")

            # ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° .env ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô mono/24kHz
            ref_path_orig = self.ref_audio_path if os.path.exists(self.ref_audio_path) else self._get_silent_reference()
            ref_path = self._prepare_reference_audio(ref_path_orig)
            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ref_text ‡πÉ‡∏ô .env ‡πÉ‡∏´‡πâ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå
            ref_text_final = (self.ref_text or self._transcribe_thai(ref_path) or "").strip()
            if self.use_reference:
                logger.info(f"üéôÔ∏è ‡πÉ‡∏ä‡πâ reference (audio='{ref_path}', text='{(ref_text_final or '[auto-th]').strip()[:30]}')")
            else:
                logger.info(f"üîß ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ reference ‡∏ï‡∏≤‡∏° config (audio='{ref_path}')")

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏¥‡∏ß‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô
            generated_audio = self.tts.infer(ref_path, ref_text_final, text)

            # ensure numpy
            try:
                if isinstance(generated_audio, torch.Tensor):
                    generated_audio = generated_audio.detach().cpu().float().numpy()
                elif isinstance(generated_audio, (list, tuple)):
                    generated_audio = np.asarray(generated_audio, dtype=np.float32).reshape(-1)
                else:
                    generated_audio = np.asarray(generated_audio, dtype=np.float32)
            except Exception:
                pass

            # Clean audio
            audio_data = self._clean_audio(generated_audio)
            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö ‡πÉ‡∏´‡πâ fallback ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ reference ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏á‡∏µ‡∏¢‡∏ö
            if np.max(np.abs(audio_data)) < 1e-6:
                logger.warning("‚ö†Ô∏è ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö ‡∏•‡∏≠‡∏á‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ reference")
                try:
                    generated_audio = self.tts.infer(self._get_silent_reference(), "", text)
                    audio_data = self._clean_audio(generated_audio)
                except Exception as e:
                    logger.error(f"‚ùå fallback non-reference ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
                    return self._generate_silence(1.0)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô WAV bytes
            wav_bytes = self._to_wav_bytes(audio_data, self.sample_rate)
            # ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö ‡πÉ‡∏´‡πâ fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Edge-TTS (‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡πà‡∏≠‡∏ô)
            if np.max(np.abs(audio_data)) < 1e-6:
                logger.warning("‚ö†Ô∏è ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏´‡∏•‡∏±‡∏á fallback non-reference ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Edge-TTS ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢")
                try:
                    wav_bytes = self._synthesize_with_edge_tts(text)
                    if wav_bytes and len(wav_bytes) > 0:
                        logger.info(f"‚úÖ Edge-TTS fallback ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(wav_bytes)} bytes")
                        return wav_bytes
                except Exception as e:
                    logger.error(f"‚ùå Edge-TTS fallback ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
            
            logger.info(f"‚úÖ ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(wav_bytes)} bytes")
            return wav_bytes

        except Exception as e:
            logger.error(f"‚ùå F5-TTS synthesis error: {e}", exc_info=True)
            # ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Discord ‡πÄ‡∏•‡πà‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á
            return self._generate_silence(1.0)

    def _clean_audio(self, audio: np.ndarray) -> np.ndarray:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á"""
        # ‡∏•‡∏ö NaN/Inf
        audio = np.nan_to_num(audio, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Clip
        audio = np.clip(audio, -1.0, 1.0)
        
        # RMS Normalize
        rms = np.sqrt(np.mean(audio**2))
        if rms > 1e-6:
            target_rms = 0.2  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏¢‡∏¥‡∏ô‡∏ä‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
            audio = audio * (target_rms / rms)
        
        # Fade in/out (10ms)
        fade_samples = int(self.sample_rate * 0.01)
        if len(audio) > fade_samples * 2:
            fade_in = np.linspace(0, 1, fade_samples)
            fade_out = np.linspace(1, 0, fade_samples)
            audio[:fade_samples] *= fade_in
            audio[-fade_samples:] *= fade_out
        
        return audio

    def _to_wav_bytes(self, audio: np.ndarray, sample_rate: int) -> bytes:
        """‡πÅ‡∏õ‡∏•‡∏á numpy array ‡πÄ‡∏õ‡πá‡∏ô WAV bytes"""
        buffer = BytesIO()
        
        audio_tensor = torch.from_numpy(audio).float()
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        
        torchaudio.save(buffer, audio_tensor, sample_rate, format="wav")
        buffer.seek(0)
        
        return buffer.read()

    def _prepare_reference_audio(self, ref_path: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô mono/24kHz ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ~3-6 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ"""
        try:
            if not os.path.exists(ref_path):
                return ref_path
            wav, sr = torchaudio.load(ref_path)
            # to mono
            if wav.shape[0] > 1:
                wav = wav.mean(dim=0, keepdim=True)
            # resample to target
            target_sr = self.sample_rate
            if sr != target_sr:
                resampler = torchaudio.transforms.Resample(sr, target_sr)
                wav = resampler(wav)
                sr = target_sr
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ faster-whisper
            try:
                seg_wav = self._extract_thai_segment_wav(wav, sr)
                if seg_wav is not None:
                    wav = seg_wav
            except Exception:
                pass
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ~6s
            max_len = int(sr * 6.0)
            if wav.shape[1] > max_len:
                wav = wav[:, :max_len]
            # write temp
            out_path = os.path.join(os.getcwd(), "temp_ref_prepared.wav")
            torchaudio.save(out_path, wav, sr)
            return out_path
        except Exception:
            # ‡∏´‡∏≤‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            return ref_path

    def _extract_thai_segment_wav(self, wav: torch.Tensor, sr: int) -> torch.Tensor | None:
        """‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤ segment ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á ~3-6 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ tensor mono ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö ‡∏°‡∏¥‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏∑‡∏ô None
        """
        try:
            import tempfile
            import re
            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô wav ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ faster-whisper‡∏≠‡πà‡∏≤‡∏ô
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            try:
                torchaudio.save(tmp.name, wav, sr)
            finally:
                tmp.flush(); tmp.close()
            from faster_whisper import WhisperModel
            model_name = os.getenv('WHISPER_MODEL', 'base')
            device = os.getenv('WHISPER_DEVICE', 'cpu')
            model = WhisperModel(model_name, device=device)
            segments, info = model.transcribe(tmp.name, language=os.getenv('WHISPER_LANG', 'th'))
            thai_re = re.compile(r"[\u0E00-\u0E7F]")
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ã‡∏Å‡πÄ‡∏°‡∏ô‡∏ï‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß > 1.0s
            chosen = None
            for seg in segments:
                if thai_re.search(seg.text or "") and (seg.end - seg.start) >= 1.0:
                    chosen = seg
                    break
            if not chosen:
                return None
            start_s = max(0.0, chosen.start - 0.2)
            end_s = min(wav.shape[1] / sr, chosen.end + 0.2)
            start_i = int(start_s * sr)
            end_i = int(end_s * sr)
            return wav[:, start_i:end_i]
        except Exception:
            return None

    def _transcribe_thai(self, audio_path: str) -> str:
        """‡∏ñ‡∏≠‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ Faster-Whisper ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        ‡∏´‡∏≤‡∏Å‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        """
        try:
            from faster_whisper import WhisperModel
            model_name = os.getenv('WHISPER_MODEL', 'base')
            device = os.getenv('WHISPER_DEVICE', 'cpu')
            model = WhisperModel(model_name, device=device)
            segments, info = model.transcribe(audio_path, language=os.getenv('WHISPER_LANG', 'th'))
            text = ''.join(seg.text for seg in segments).strip()
            if text:
                logger.info(f"üìù Thai ref_text from Faster-Whisper: {text[:50]}")
            else:
                logger.info("üìù Thai ref_text empty from Faster-Whisper")
            return text
        except Exception as e:
            logger.info(f"‚ö†Ô∏è Thai transcription failed, will let TTS auto-transcribe: {e}")
            return ""

    def _get_silent_reference(self) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reference"""
        silent_path = "temp_silent_ref.wav"
        
        if not os.path.exists(silent_path):
            duration = 0.5
            silent_audio = np.zeros(int(self.sample_rate * duration), dtype=np.float32)
            silent_tensor = torch.from_numpy(silent_audio).unsqueeze(0)
            torchaudio.save(silent_path, silent_tensor, self.sample_rate)
            logger.info(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏á‡∏µ‡∏¢‡∏ö: {silent_path}")
        
        return silent_path

    def _generate_silence(self, duration: float) -> bytes:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏á‡∏µ‡∏¢‡∏ö"""
        silent_audio = np.zeros(int(self.sample_rate * duration), dtype=np.float32)
        return self._to_wav_bytes(silent_audio, self.sample_rate)

    def _synthesize_with_edge_tts(self, text: str) -> bytes:
        """‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡πâ‡∏ß‡∏¢ Edge-TTS (‡∏ú‡πà‡∏≤‡∏ô CLI) ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô WAV 24kHz mono ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ"""
        # ‡πÉ‡∏ä‡πâ CLI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ event loop
        voice = os.getenv("EDGE_TTS_VOICE", "th-TH-AcharaNeural")
        rate = os.getenv("EDGE_TTS_RATE", "+10%")
        pitch = os.getenv("EDGE_TTS_PITCH", "+150Hz")
        ffmpeg_bin = os.getenv("FFMPEG_BINARY", "ffmpeg")

        tmp_dir = os.path.join(os.getcwd(), "temp")
        os.makedirs(tmp_dir, exist_ok=True)
        mp3_path = os.path.join(tmp_dir, "edge_fallback.mp3")
        wav_path = os.path.join(tmp_dir, "edge_fallback.wav")

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ edge-tts CLI ‡∏ú‡πà‡∏≤‡∏ô python -m ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£ await
        import sys
        cmd = [
            sys.executable,
            "-m", "edge_tts",
            "--text", text,
            "--voice", voice,
            "--rate", rate,
            "--pitch", pitch,
            "--write-media", mp3_path,
        ]
        try:
            subprocess.check_call(cmd)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"edge-tts CLI failed: {e}")

        # ‡πÅ‡∏õ‡∏•‡∏á MP3 -> WAV 24kHz mono
        try:
            subprocess.check_call([
                ffmpeg_bin,
                "-y",
                "-i", mp3_path,
                "-ar", str(self.sample_rate),
                "-ac", "1",
                wav_path,
            ])
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"ffmpeg convert failed: {e}")

        with open(wav_path, "rb") as rf:
            return rf.read()