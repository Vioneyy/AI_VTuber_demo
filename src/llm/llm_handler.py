"""
‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ LLM (ChatGPT) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û
‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á: src/llm/llm_handler.py (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà chatgpt_client.py)
‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå: prompts/system_prompt.txt (‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô jeed_persona.py ‡πÅ‡∏•‡πâ‡∏ß)
"""

import asyncio
import time
from typing import Optional, List, Dict
import openai
from openai import AsyncOpenAI

import sys
sys.path.append('..')
from core.config import config
from personality.jeed_persona import JeedPersona, Emotion

class LLMHandler:
    """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ LLM ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=config.llm.api_key)
        self.conversation_history: List[Dict] = []
        self.max_history = 10  # ‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ 10 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        
        # Statistics
        self.total_requests = 0
        self.total_tokens = 0
        self.avg_response_time = 0
        
    async def generate_response(self, user_message: str, retry: int = 2) -> Optional[str]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å LLM
        Args:
            user_message: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
            retry: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà
        Returns:
            ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
        """
        start_time = time.time()
        
        try:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })
            
            # ‡∏ï‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏Å‡πà‡∏≤
            if len(self.conversation_history) > self.max_history * 2:
                self.conversation_history = self.conversation_history[-self.max_history * 2:]
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á messages
            messages = [
                {"role": "system", "content": JeedPersona.SYSTEM_PROMPT}
            ] + self.conversation_history
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=config.llm.model,
                    messages=messages,
                    max_tokens=config.llm.max_tokens,
                    temperature=config.llm.temperature,
                    presence_penalty=config.llm.presence_penalty,
                    frequency_penalty=config.llm.frequency_penalty
                ),
                timeout=config.llm.timeout
            )
            
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            assistant_message = response.choices[0].message.content.strip()
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            assistant_message = JeedPersona.clean_response(assistant_message)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            is_valid, error = JeedPersona.validate_response(assistant_message)
            
            if not is_valid:
                print(f"‚ö†Ô∏è ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {error}")
                if retry > 0:
                    print(f"üîÑ ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà... (‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {retry} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)")
                    return await self.generate_response(user_message, retry - 1)
                else:
                    assistant_message = "‡πÄ‡∏≠‡πä‡∏∞ ‡∏´‡∏ô‡∏π‡∏á‡∏á‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°~"
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
            self.conversation_history.append({
                "role": "assistant",
                "content": assistant_message
            })
            
            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            elapsed = time.time() - start_time
            self.total_requests += 1
            self.total_tokens += response.usage.total_tokens
            self.avg_response_time = (
                (self.avg_response_time * (self.total_requests - 1) + elapsed) 
                / self.total_requests
            )
            
            print(f"ü§ñ LLM Response ({elapsed:.2f}s, {response.usage.total_tokens} tokens)")
            print(f"   '{assistant_message[:100]}...'")
            
            return assistant_message
            
        except asyncio.TimeoutError:
            print(f"‚è∞ LLM Timeout ({config.llm.timeout}s)")
            if retry > 0:
                return await self.generate_response(user_message, retry - 1)
            return "‡πÄ‡∏≠‡πä‡∏∞ ‡∏´‡∏ô‡∏π‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤‡πÑ‡∏õ‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏ô‡∏∞ ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°~"
            
        except openai.APIError as e:
            print(f"‚ùå OpenAI API Error: {e}")
            if retry > 0:
                await asyncio.sleep(1)
                return await self.generate_response(user_message, retry - 1)
            return "‡∏≠‡∏∏‡πä‡∏õ‡∏™‡πå ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ‡∏ô‡∏∞~"
            
        except Exception as e:
            print(f"‚ùå LLM Error: {e}")
            return "‡πÄ‡∏≠‡πä‡∏∞ ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏ô‡∏∞~"
    
    def clear_history(self):
        """‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤"""
        self.conversation_history = []
        print("üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
    
    def get_stats(self) -> Dict:
        """‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"""
        return {
            "total_requests": self.total_requests,
            "total_tokens": self.total_tokens,
            "avg_response_time": self.avg_response_time,
            "history_size": len(self.conversation_history)
        }
    
    def print_stats(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"""
        stats = self.get_stats()
        print("\n" + "="*50)
        print("ü§ñ LLM Statistics")
        print("="*50)
        print(f"Total Requests: {stats['total_requests']}")
        print(f"Total Tokens: {stats['total_tokens']}")
        print(f"Avg Response Time: {stats['avg_response_time']:.2f}s")
        print(f"History Size: {stats['history_size']}")
        print("="*50 + "\n")

# Global LLM handler
llm_handler = LLMHandler()